{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "seed = 2146\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = 8\n",
    "bits_vocab_len = 2**bits\n",
    "\n",
    "print(f\"bits vocab len: {bits_vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\" #gpt2-medium / gpt2-large /gpt2-xl\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = TokenMapper(bits_vocab_len, llm_feature_dim, device=device)\n",
    "reverseMapper = TokenMapper(bits_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "\n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions_withfv(token_fv):\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(inputs_embeds=token_fv, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings, temperature=1.0):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "    \n",
    "\n",
    "    cosine_similarities = torch.matmul(batch_feature_vectors, embeddings.T)\n",
    "    sfmx = torch.softmax(cosine_similarities/temperature, dim=2)\n",
    "    closest_tokens = torch.argmax(sfmx, dim=2)\n",
    "    mm = torch.matmul(sfmx, embeddings)\n",
    "\n",
    "    return mm, cosine_similarities, closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = 8\n",
    "batch_size = 1\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, directories, filetypes=\".jpg\",transform=None, seq_len=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            directory (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.directories = directories\n",
    "        self.transform = transform\n",
    "        # self.filenames = [f for f in os.listdir(directory) if f.endswith(filetype)]\n",
    "\n",
    "        self.filenames = []\n",
    "        for directory in directories:\n",
    "            # Store the full path to each file\n",
    "            full_paths = [os.path.join(directory, f) for f in os.listdir(directory) \n",
    "                               if any(f.endswith(filetype) for filetype in filetypes)]\n",
    "            self.filenames.extend(full_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct the full path to the image file\n",
    "        path = self.filenames[idx]\n",
    "        \n",
    "        # Read the image file as bytes\n",
    "        with open(path, 'rb') as file:\n",
    "            b = file.read()\n",
    "\n",
    "        # If a transform is specified, apply it\n",
    "        if self.transform:\n",
    "            b = self.transform(b)\n",
    "        \n",
    "        return b, self.filenames[idx]\n",
    "\n",
    "class ToBinaryString:\n",
    "    def __init__(self, bits=8, segment_length=256):\n",
    "        self.bits = bits  # Number of bits to group together into an integer\n",
    "        self.chunk_size = segment_length  # Number of integers per segment\n",
    "\n",
    "    def __call__(self, image_bytes):\n",
    "        # Convert bytes to binary string\n",
    "\n",
    "        binary_string = ''.join(f'{byte:08b}' for byte in image_bytes)\n",
    "\n",
    "        integers = [int(binary_string[i:i+self.bits], 2) for i in range(0, len(binary_string), self.bits)]\n",
    "        tensor = torch.tensor(integers)\n",
    "\n",
    "        padding_size = (self.chunk_size - tensor.size(0) % self.chunk_size) % self.chunk_size\n",
    "\n",
    "        # Pad the tensor if necessary\n",
    "        if padding_size > 0:\n",
    "            tensor = torch.cat([tensor, torch.zeros(padding_size, dtype=tensor.dtype)])\n",
    "\n",
    "        # Reshape the tensor into chunks of chunk_size\n",
    "        # Ensure the total length is divisible by chunk_size before reshaping\n",
    "        total_length = tensor.size(0) + padding_size\n",
    "        tensor = tensor.view(-1, self.chunk_size)\n",
    "\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch, seq_len = 256):\n",
    "    # Find the maximum number of chunks in this batch\n",
    "    max_chunks = max([x[0].size(0) for x in batch])\n",
    "    \n",
    "    # Pad each item in the batch to this size\n",
    "    padded_batch = []\n",
    "    for tensors, filename in batch:\n",
    "        if tensors.size(0) < max_chunks:\n",
    "            pad_size = (max_chunks - tensors.size(0)) * seq_len\n",
    "            padded_tensors = torch.cat([tensors, torch.zeros(pad_size, dtype=tensors.dtype).view(-1, seq_len)])\n",
    "        else:\n",
    "            padded_tensors = tensors\n",
    "        padded_batch.append((padded_tensors, filename))\n",
    "    \n",
    "    # Stack all the tensors together along a new 0th dimension, and return filenames separately\n",
    "    tensors, filenames = zip(*padded_batch)\n",
    "    return torch.stack(tensors), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Initialize the dataset\n",
    "dirs = ['../data/enwik9/ascii/', '../data/imagenet/train/png_small/', '../data/librispeech/train/wav/']\n",
    "filetypes = ['.txt', '.png', '.wav']\n",
    "\n",
    "dataset = BinaryDataset(directories=dirs, filetypes=filetypes, transform=ToBinaryString(segment_length=seq_len))\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Setup the DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "testloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 5e-6\n",
    "epochs = 1\n",
    "gamma = 0.1\n",
    "temperature = 0.001\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"test\"\n",
    "algo = \"base\"\n",
    "exp_type = \"hybrid\"\n",
    "name = f\"{bits}bits\"\n",
    "experiment_name = f\"{exp_type}/{algo}/{experiment}/{name}/{llm}/lr={learning_rate}/gamma={gamma}/temp={temperature}/promptlen={prompt_len}/seq_len={seq_len}\"\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs_test/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(mapper.parameters()) + list(reverseMapper.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    reverseMapper.train()\n",
    "    for i, dd in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        data = dd[0]\n",
    "\n",
    "        ground_truth_tokens = data.reshape(-1, seq_len).to(device)\n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=bits_vocab_len).float()\n",
    "\n",
    "        # Logits are to be compared with the next ground truth tokens\n",
    "        ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "        inputs_feature_vector = mapper(one_hot_tokens)\n",
    "        \n",
    "        # Map tokens and get ground truth from LLM\n",
    "        translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "        \n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "        logits = logits[:,:-1]\n",
    "        logits_ = logits.reshape(-1, bits_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)        \n",
    "        ce_loss = criterion(logits_, ground_truth_tokens)\n",
    "        \n",
    "        writer.add_scalar(\"training/cross_entropy\", ce_loss.item(), global_step)\n",
    "        ce_loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        if global_step%100==0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {global_step}, CE Loss: {ce_loss.mean().item()}\")\n",
    "        global_step+=1\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{experiment_name}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}/mapper.pt\")\n",
    "torch.save(reverseMapper.state_dict(), f\"../models/{experiment_name}/reversemapper.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models (Optional)\n",
    "\n",
    "mapper.load_state_dict(torch.load(f\"../models/{experiment_name}/mapper.pt\"))\n",
    "reverseMapper.load_state_dict(torch.load(f\"../models/{experiment_name}/reversemapper.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "total = []\n",
    "mapper.eval()\n",
    "reverseMapper.eval()\n",
    "for i, dd in enumerate(testloader):\n",
    "\n",
    "    data = dd[0]\n",
    "    \n",
    "    ground_truth_tokens = data.reshape(-1, seq_len).to(device)\n",
    "    one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=bits_vocab_len).float()\n",
    "\n",
    "    # Logits are to be compared with the next ground truth tokens\n",
    "    ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "    inputs_feature_vector = mapper(one_hot_tokens)\n",
    "\n",
    "    # Map tokens and get ground truth from LLM\n",
    "    translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "\n",
    "    # Calculate Representation of Last Layer in LLM\n",
    "    final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "\n",
    "    # Calculate Logits with mapper function\n",
    "    logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "    # logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "    logits = logits[:,:-1]\n",
    "    logits_ = logits.reshape(-1, bits_vocab_len)\n",
    "    ground_truth_tokens = ground_truth_tokens.reshape(-1)        \n",
    "    ce_loss = criterion(logits_, ground_truth_tokens)\n",
    "\n",
    "    total.append(ce_loss.item())\n",
    "    if global_step%100==0:\n",
    "        print(f\" Batch {global_step}, CE Loss: {ce_loss.mean().item()}\")\n",
    "    global_step+=1\n",
    "\n",
    "    if global_step % 100 == 0:\n",
    "        break\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "testing = np.array(total)\n",
    "\n",
    "print(testing.mean())\n",
    "print(testing.std())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
